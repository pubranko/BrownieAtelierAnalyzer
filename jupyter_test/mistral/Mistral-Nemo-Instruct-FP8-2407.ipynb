{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# url = \"http://localhost:8000/v1/chat/completions\"\n",
    "url = \"http://localhost:8901/v1/chat/completions\"\n",
    "headers = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\n",
    "\n",
    "model = \"mistralai/Mistral-Nemo-Instruct-FP8-2407\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How expensive would it be to ask a window cleaner to clean all windows in Paris. Make a reasonable guess in US Dollar.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "data = {\"model\": model, \"messages\": messages}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "print(response.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "コマンドでサーバー起動\n",
    "    vllm serve mistralai/Mistral-Nemo-Instruct-FP8-2407 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral\n",
    "\n",
    "コマンドの全体像\n",
    "    vllm serve\n",
    "        vLLM（大規模言語モデル用の高速推論サーバー）を起動し、指定したモデルでAPIサーバーとして提供します。OpenAI互換API（/v1/completions, /v1/chat/completionsなど）を標準で提供します。\n",
    "    mistralai/Mistral-Nemo-Instruct-FP8-2407\n",
    "        サーバーで提供するモデル名。Mistral AIとNVIDIAが共同開発した命令追従型・多言語対応の大規模言語モデルで、FP8（8ビット浮動小数点）量子化済みバージョンです。\n",
    "\n",
    "主なオプションの意味\n",
    "    --tokenizer_mode mistral\n",
    "        トークナイザー（テキストをトークンに分割する処理）の方式として「mistral」方式を指定します。\n",
    "        モデルに合わせたトークナイザーを使うことで、正しい入出力が保証されます。\n",
    "    --config_format mistral\n",
    "        モデルの設定ファイル（config）の読み込みフォーマットとして「mistral」形式を指定します。\n",
    "        Mistral系モデル特有の設定ファイル構造に対応します。\n",
    "    --load_format mistral\n",
    "        モデルの重みファイル（パラメータ）の読み込みフォーマットとして「mistral」形式を指定します。\n",
    "        これもMistral系モデル特有の保存形式に合わせるためです。\n",
    "    --tool-call-parser mistral\n",
    "        ツール呼び出し（Tool Call）機能を利用する際のパーサーとして「mistral」方式を指定します。\n",
    "        Mistralモデルがサポートするツール呼び出し仕様に従ってAPI経由で外部ツール連携が可能になります。\n",
    "\n",
    "実行時の動作\n",
    "    指定モデル（mistralai/Mistral-Nemo-Instruct-FP8-2407）をダウンロード・GPUにロードし、APIサーバーとして起動します。\n",
    "    デフォルトでは http://localhost:8000 でAPIリクエストを受け付けます。\n",
    "\n",
    "OpenAI互換のエンドポイント（/v1/completions, /v1/chat/completionsなど）を使って、外部アプリやcurl、OpenAI SDKなどから推論リクエストが可能です。\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00b0d91d220cd2884303810c80f143c1222c3c3704eaa0756460e122a00ee18a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
